= Benchmarking Parallel Programs

=== Testing and Benchmarking

테스트와 벤치마킹은 다르다.

* testing - ensures that parts of the program are behaving according to the intended behavior
** ex) reverse method is what we expect

* benchmarking - computes performance metrics for parts of the program
** ex) running time, memory footprint, metric traffic, disk usage, latency

Typically testing yields a binary output - a program or its part is either correct or it is not

Benchmarking usually yields a continuous value, which denotes the extent to which the program is correct


=== Benchmarking Parallel Programs

왜 우리는 병렬 프로그램을 벤치마킹해 보아야 하는가?

주된 이유는 성능 이점이 실제로 수행시간에 향상되었는지를 알아봐야 하기 때문이다.

Benchmarking parallel programs is even more important than benchmarking sequential programs

만약 성능상에 이점이 없다면 순차 프로그램으로 계속 작성할 수 있다. 이게 더 쉽게 만들수 있고 이해하기도 쉽다. 이런 이유에서 병렬 프로그램을 벤치마킹해보는 것은 순차 프로그램을 벤치마킹하는 것보다 더 중요하다.

=== Performance Factors

- processor speed
- number of processors
- memory access latency and throughput (affects contention)
- cache behavior(e.g. false sharing, associativity effects)
- runtime behavior (e.g. garbage collection, JIT compilation, thread scheduling)

To learn more, see link:../static/docs/cpumemory.pdf[What Every Programmer Should Know About Momory, by Ulrich Drepper]

=== Measurement Methodologies

Measuring performance is difficult - usually, the a performance metric is a random variable.

- multiple repetitions
- statistical treatment - computing mean and variance
- eliminating outliers
- ensuring steady state(warm-up)
- preventing anomalies(GC, JIT compilation, aggressive optimizations)

To learn more, see link:../static/docs/oopsla07-georges.pdf[Statistically Rigorous Java Performance Evaluation, by Georges, Buytaert, and Eechhout]

== ScalaMeter

ScalaMeter is a benchmarking and performance regression testing framework for the JVM

- performance regression testing - comparing performance of the current program run against known previous runs
- benchmarking - measuring performance of the current (part of the) program

[source,scala]
----
libraryDependencies += "com.storm-enroute" %% "scalameter-core" % 0.6

import org.scalameter._

val time = measure {
  (0 until 10000).toArray
}

println(s"Array initialization time $time ms")
----

수행해보면 가끔 시간이 튀는 경우가 있는데 GC가 수행되어서 이다.

=== JVM Warmup

The demo showed two very different running times on two consecutive runs of the program.

When a JVM program starts, it undergoes a period of warmup, after which it achieves its maximum performance.

- first, the program is interpreted
- then, parts of the program are compiled into machine code
- later, the JVM may choose to apply additional dynamic optimizations
- eventually, the program reaches steady state

=== ScalaMeter Warmers

Usually, we want to measure steady state program performance.

ScalaMeter Warmer objects run the benchmarked code until detecting steady state.

[source,scala]
----
import org.scalameter._

val time = withWarmer(new Warmer.Default) measure {
  (0 until 1000000).toArray
}
----

=== ScalaMeter Configuration

ScalaMeter configuration clause allows specifying various parameters, such as the minimum and maximum number of warmup runs.

[source,scala]
----
val time = config(
  Key.exec.minWarmupRuns -> 20,
  Key.exec.masWarmupRuns -> 60,
  Key.verbose -> true
) withWarmer(new Warmer.Default) measure {
  (0 until 1000000).toArray
}
----

=== ScalaMeter Measurers

- Measurer.Default - plain running time
- IgnoringGC - running time without GC pauses
- OutlierEliminations - removes statistical outliers
- MemoryFootprint - memory footpring of an object
- GarbageCollectionsCycles - total number of GC pauses
- newer ScalaMeter version can also measure method invocation counts and boxing counts

[source,scala]
----
withMeasurer(new Measurer.MemoryFootprint) measure{ (0 until 1000000).toArray }
----
